# DeepLearning-Ground-Up
This project demonstrates how to build core modern deep learning components without relying on high-level PyTorch abstractions.

### ðŸ“š Series Contents

- **[Loss Functions from Scratch](./Markdown/01-loss-functions.md/)** - Cross-entropy, MSE, focal loss, and more
- **[Activation Functions from Scratch](./Markdown/02-activation-functions.md/)** - ReLU, GELU, Swish, and advanced variants
- **[Attention Mechanisms from Scratch](./Markdown/03-attention-mechanisms.md/)** - Scaled dot-product Attention, Multi-Head Attention
- **[Positional Encodings from Scratch](./Markdown/04-positional-encodings.md/)** - Sinusoidal, relative encodings, RoPE
- **[Normalization & Regularization from Scratch](./Markdown/05-normalization-regularization.md/)** - LayerNorm, BatchNorm, Dropout, and more
- **[Linear Layers from Scratch](./Markdown/06-linear-layers.md/)** - Dense layers and weight initialization
- **[Embedding Layers from Scratch](./Markdown/07-embedding-layers.md/)** - Token embeddings
- **[Transformer Architecture from Scratch](./Markdown/08-transformer.md/)** - Complete Transformer implementation with encoder-decoder architecture

### ðŸš« Constraints (Following CS336 Guidelines)
**Not Allowed:**
- `torch.nn` modules (except containers)
- `torch.nn.functional` functions
- `torch.optim` optimizers (except base class)

**Allowed:**
- `torch.nn.Parameter` for trainable parameters
- Container classes: `torch.nn.Module`, `torch.nn.ModuleList`, `torch.nn.Sequential`
- `torch.optim.Optimizer` base class for custom optimizers
- Core PyTorch tensor operations

---

æœ¬é¡¹ç›®å±•ç¤ºå¦‚ä½•åœ¨ä¸ä¾èµ– PyTorch é«˜çº§æŠ½è±¡çš„æƒ…å†µä¸‹æž„å»ºæ ¸å¿ƒçŽ°ä»£æ·±åº¦å­¦ä¹ ç»„ä»¶ã€‚

### ðŸ“š ç³»åˆ—å†…å®¹

- **[ä»Žé›¶å®žçŽ°æŸå¤±å‡½æ•°](./Markdown/01-loss-functions.md/)** - äº¤å‰ç†µã€å‡æ–¹è¯¯å·®ã€ç„¦ç‚¹æŸå¤±ç­‰
- **[ä»Žé›¶å®žçŽ°æ¿€æ´»å‡½æ•°](./Markdown/02-activation-functions.md/)** - ReLUã€GELUã€Swish åŠé«˜çº§å˜ä½“
- **[ä»Žé›¶å®žçŽ°æ³¨æ„åŠ›æœºåˆ¶](./Markdown/03-attention-mechanisms.md/)** - ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ã€å¤šå¤´æ³¨æ„åŠ›
- **[ä»Žé›¶å®žçŽ°ä½ç½®ç¼–ç ](./Markdown/04-positional-encodings.md/)** - æ­£å¼¦ç¼–ç ã€ç›¸å¯¹ç¼–ç ã€æ—‹è½¬ä½ç½®ç¼–ç (RoPE)
- **[ä»Žé›¶å®žçŽ°å½’ä¸€åŒ–å’Œæ­£åˆ™åŒ–](./Markdown/05-normalization-regularization.md/)** - å±‚å½’ä¸€åŒ–ã€æ‰¹å½’ä¸€åŒ–ã€Dropout ç­‰
- **[ä»Žé›¶å®žçŽ°çº¿æ€§å±‚](./Markdown/06-linear-layers.md/)** - å…¨è¿žæŽ¥å±‚å’Œæƒé‡åˆå§‹åŒ–
- **[ä»Žé›¶å®žçŽ°åµŒå…¥å±‚](./Markdown/07-embedding-layers.md/)** - è¯åµŒå…¥
- **[ä»Žé›¶å®žçŽ° Transformer æž¶æž„](./Markdown/08-transformer.md/)** - å®Œæ•´çš„ç¼–ç å™¨-è§£ç å™¨ Transformer å®žçŽ°

### ðŸš« å®žçŽ°è¦æ±‚ï¼ˆéµå¾ª CS336 å‡†åˆ™ï¼‰
**ä¸å…è®¸ä½¿ç”¨:**
- `torch.nn` æ¨¡å—ï¼ˆå®¹å™¨ç±»é™¤å¤–ï¼‰
- `torch.nn.functional` å‡½æ•°
- `torch.optim` ä¼˜åŒ–å™¨ï¼ˆåŸºç±»é™¤å¤–ï¼‰

**å…è®¸ä½¿ç”¨:**
- `torch.nn.Parameter` ç”¨äºŽå¯è®­ç»ƒå‚æ•°
- å®¹å™¨ç±»: `torch.nn.Module`ã€`torch.nn.ModuleList`ã€`torch.nn.Sequential`
- `torch.optim.Optimizer` åŸºç±»ç”¨äºŽè‡ªå®šä¹‰ä¼˜åŒ–å™¨
- PyTorch æ ¸å¿ƒå¼ é‡æ“ä½œ

> Zhihu Link: https://www.zhihu.com/column/c_1930370026797503503