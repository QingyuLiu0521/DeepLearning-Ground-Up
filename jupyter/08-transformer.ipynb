{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 八、手撕Transformer\n",
    "<img src=\"../images/TransformerArchitecture.png\" width=\"300\">\n",
    "\n",
    "本章代码中Layer Normlization的放置位置使用 \"[Attention Is All You Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\" 中最经典的 Post Norm：$$x_{t+1} = \\text{Norm}(x_{t}+F_t(x_t))$$关于Pre Norm和Post Norm的差异已经在\"[五、手撕归一化和正则化](https://zhuanlan.zhihu.com/p/1941315405210719817)\"介绍。\n",
    "\n",
    "位置编码使用出自 \"[RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)\" 的旋转位置编码（RoPE）。\n",
    "\n",
    "前馈神经网络子层(FFN)的激活函数使用GeLU:$$\\begin{aligned}\\text{GeLU}(x) &= x \\cdot \\Phi(x)\\\\&\\approx 0.5x\\left(1 + \\tanh\\left(\\sqrt{2/\\pi}(x + 0.044715x^3)\\right)\\right)\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 核心组件实现\n",
    "下面是核心组件 包括RoPE, MHA, Layer Norm, Dropout, GeLU, Linear, FFN, Embedding 的实现；这些组件已经在前几章进行详细介绍:\n",
    "\n",
    "- RoPE: [手撕位置编码](https://zhuanlan.zhihu.com/p/1933614057816032698)\n",
    "- MHA: [手撕注意力机制](https://zhuanlan.zhihu.com/p/1931817125980373592)\n",
    "- Layer Norm, Dropout: [手撕归一化和正则化](https://zhuanlan.zhihu.com/p/1941315405210719817)\n",
    "- GeLU: [手撕激活函数](https://zhuanlan.zhihu.com/p/1931808435873255820)\n",
    "- Linear: [手撕线性层](https://zhuanlan.zhihu.com/p/1941317825613500636)\n",
    "- Embedding: [手撕Embedding层](https://zhuanlan.zhihu.com/p/1945454321241138838)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "# 基本组件：RoPE, MHA, LN, GeLU, FFN\n",
    "# 计算旋转矩阵\n",
    "def precompute_freqs_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    # 根据dim计算θi\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    # 生成序列索引（0 ~ seq_len-1）\n",
    "    t = torch.arange(seq_len, device=freqs.device)\n",
    "    # torch.outer(): 计算两个向量的外积; 它会返回一个矩阵，其中每个元素是第一个向量的元素与第二个向量的元素的乘积。\n",
    "    # 计算每个位置、每个频率对的最终旋转角度 m * θ_i\n",
    "    freqs = torch.outer(t, freqs).float()  # shape = [seq_len, dim // 2]\n",
    "\n",
    "    # torch.polar：根据极坐标（模长和角度），生成一个复数 e^(i * angle)\n",
    "    # 每个复数的模长为1，由torch.ones_like提供\n",
    "    # 角度为freqs = m * θ_i\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)   # shape = [seq_len, dim // 2]\n",
    "    return freqs_cis\n",
    "\n",
    "# 旋转位置编码计算\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # input.shape: [batch_size, n_heads, seq_len, d_k]\n",
    "    # 将d_k拆成d_k // 2 和 2\n",
    "    xq_ = xq.float().reshape(*xq.shape[:-1], -1, 2) # shape = [batch_size, n_heads, seq_len, dim//2, 2]\n",
    "    xk_ = xk.float().reshape(*xk.shape[:-1], -1, 2) # shape = [batch_size, n_heads, seq_len, dim//2, 2]\n",
    "    \n",
    "    # torch.view_as_complex将 [real, imag] 对转换为一个真正的复数\n",
    "    # 例：[1, 2] -> 1 + 2j\n",
    "    # shape: [B, H, S, D_k/2, 2] -> [B, H, S, D_k/2]\n",
    "    # 维度虽然减少，但最后一个维度现在是complex64类型\n",
    "    xq_ = torch.view_as_complex(xq_) # shape = [batch_size, n_heads, seq_len, dim//2]\n",
    "    xk_ = torch.view_as_complex(xk_) # shape = [batch_size, n_heads, seq_len, dim//2]\n",
    "\n",
    "    # torch.view_as_real 将旋转后的复数张量再转换回实数表示，shape:  [B, H, S, D_k/2] -> [B, H, S, D_k/2, 2]\n",
    "    # .flatten(3): 从第3个维度开始展平，即把最后两个维度[d_k/2, 2]合并，恢复原始的 d_k 维度\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3) # shape = [batch_size, n_heads, seq_len, d_k]\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3) # shape = [batch_size, n_heads, seq_len, d_k]\n",
    "    \n",
    "    # 输出张量的数据类型与输入类型完全一致\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "# 多头注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, max_seq_len=512, dropout_ratio=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        self.freqs_cis = precompute_freqs_cis(self.d_k, max_seq_len * 2)\n",
    "         \n",
    "    def forward(self, x, causal_mask=None, padding_mask=None):\n",
    "        batch_size, seq_len , _ = x.shape\n",
    "        \n",
    "        Q = self.linear_q(x)    # shape: [batch_size, seq_len, d_model]\n",
    "        K = self.linear_k(x)    # shape: [batch_size, seq_len, d_model]\n",
    "        V = self.linear_v(x)    # shape: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # reshape并交换seq_len和n_heads两个维度\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # shape: [batch_size, n_heads, seq_len, d_k]\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # shape: [batch_size, n_heads, seq_len, d_k]\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # shape: [batch_size, n_heads, seq_len, d_k]\n",
    "        \n",
    "        # 对Q和K应用旋转位置编码\n",
    "        Q, K = apply_rotary_emb(Q, K, freqs_cis=self.freqs_cis[:seq_len].to(x.device))  # shape: [batch_size, n_heads, seq_len, d_k]\n",
    "        \n",
    "        # 计算attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)    # shape: [batch_size, n_heads, seq_len, seq_len]\n",
    "\n",
    "        if padding_mask is not None:\n",
    "            mask = padding_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, encoder_seq_len]\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        \n",
    "        if causal_mask is not None:\n",
    "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores.masked_fill(causal_mask, -1e9)    # shape: [batch_size, n_heads, seq_len, seq_len]\n",
    "        \n",
    "        probs = torch.softmax(scores, dim = -1)\n",
    "        probs = self.dropout(probs)    # shape: [batch_size, n_heads, seq_len, seq_len]\n",
    "        \n",
    "        # 计算attention输出\n",
    "        attention = torch.matmul(probs, V) # V不应用RoPE\n",
    "        attention = attention.transpose(1, 2).reshape(batch_size, -1, self.n_heads * self.d_k)  # shape: [batch_size, seq_len, d_model]\n",
    "        output = self.linear_o(attention)  # shape: [batch_size, seq_len, d_model]\n",
    "        return output\n",
    "    \n",
    "# 交叉多头注意力机制\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_model, max_seq_len=512, dropout_ratio=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.linear_q = nn.Linear(d_model, d_model)\n",
    "        self.linear_k = nn.Linear(d_model, d_model)\n",
    "        self.linear_v = nn.Linear(d_model, d_model)\n",
    "        self.linear_o = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        self.freqs_cis = precompute_freqs_cis(self.d_k, max_seq_len * 2)\n",
    "         \n",
    "    def forward(self, decoder_x, encoder_output, encoder_attention_mask=None):\n",
    "        batch_size, decoder_seq_len, _ = decoder_x.shape\n",
    "        _, encoder_seq_len, _ = encoder_output.shape\n",
    "        \n",
    "        Q = self.linear_q(decoder_x)         # shape: [batch_size, decoder_seq_len, d_model]\n",
    "        K = self.linear_k(encoder_output)    # shape: [batch_size, encoder_seq_len, d_model]\n",
    "        V = self.linear_v(encoder_output)    # shape: [batch_size, encoder_seq_len, d_model]\n",
    "        \n",
    "        # reshape并交换seq_len和n_heads两个维度\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # shape: [batch_size, n_heads, decoder_seq_len, d_k]\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # shape: [batch_size, n_heads, encoder_seq_len, d_k]\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)  # shape: [batch_size, n_heads, encoder_seq_len, d_k]\n",
    "        \n",
    "        # decoder的cross attention不额外注入位置编码\n",
    "        \n",
    "        # 计算attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(self.d_k)    # shape: [batch_size, n_heads, decoder_seq_len, encoder_seq_len]\n",
    "        \n",
    "        # 使用encoder_attention_mask来mask掉encoder中的padding位置\n",
    "        if encoder_attention_mask is not None:\n",
    "            # encoder_attention_mask: [batch_size, encoder_seq_len] \n",
    "            # True表示有效位置，False表示padding位置\n",
    "            # 需要扩展维度并取反（因为我们要mask掉False的位置）\n",
    "            mask = encoder_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, encoder_seq_len]\n",
    "            # mask = mask.bool()\n",
    "            scores = scores.masked_fill(~mask, -1e9)\n",
    "        \n",
    "        probs = torch.softmax(scores, dim = -1)\n",
    "        probs = self.dropout(probs)    # shape: [batch_size, n_heads, decoder_seq_len, encoder_seq_len]\n",
    "        \n",
    "        # 计算attention输出\n",
    "        attention = torch.matmul(probs, V) # shape: [batch_size, n_heads, decoder_seq_len, d_k]\n",
    "        attention = attention.transpose(1, 2).reshape(batch_size, -1, self.n_heads * self.d_k)  # shape: [batch_size, decoder_seq_len, d_model]\n",
    "        output = self.linear_o(attention)  # shape: [batch_size, decoder_seq_len, d_model]\n",
    "        return output\n",
    "\n",
    "# LN\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, normalized_size, eps=1e-5, elementwise_affine=True):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_size\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "        \n",
    "        if self.elementwise_affine:\n",
    "            # nn.Parameter: 可训练参数\n",
    "            self.gamma = nn.Parameter(torch.ones(normalized_size))\n",
    "            self.beta = nn.Parameter(torch.zeros(normalized_size))\n",
    "        else:\n",
    "            self.register_parameter('gamma', None)\n",
    "            self.register_parameter('beta', None)\n",
    "    \n",
    "    def forward(self, x):   # 例：x.shape = [batch_size, seq_len, hidden_size]\n",
    "        mu = x.mean(dim=-1, keepdim=True)                   # shape: [batch_size, seq_len, 1]\n",
    "        # 计算总体方差而不是样本方差，所以unbiased=False\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)   # shape: [batch_size, seq_len, 1]\n",
    "        \n",
    "        x_norm = (x - mu) / torch.sqrt(var+self.eps)\n",
    "        if self.elementwise_affine:\n",
    "            output = self.gamma * x_norm + self.beta\n",
    "        else:\n",
    "            output = x_norm\n",
    "            \n",
    "        return output                                      # shape: [batch_size, seq_len, 1]\n",
    "    \n",
    "# Dropout\n",
    "class Dropout(nn.Module):\n",
    "    def __init__(self, p=0.1):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # torch.rand生成0~1的均匀分布，跟self.p比较就能生成掩码\n",
    "            mask = (torch.rand(x.shape) > self.p).float()\n",
    "            output = mask * x / (1 - self.p)\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "    \n",
    "# Tanh 和 GeLU\n",
    "class Tanh(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, x):\n",
    "        return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.k = 0.044715\n",
    "        self.tanh = Tanh()\n",
    "    def forward(self, x):\n",
    "        tanh = self.tanh(math.sqrt(2 / math.pi) * (x + self.k * x**3))\n",
    "        return 0.5 * x * (1 + tanh)\n",
    "\n",
    "# Linear\n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.empty(self.out_features, self.in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(self.out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = torch.matmul(x, self.weight.t())\n",
    "        if self.bias is not None:\n",
    "            output += self.bias\n",
    "        return output\n",
    "\n",
    "# FFN\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, hidden_size, ff_size, dropout_ratio=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(hidden_size, ff_size)\n",
    "        self.linear2 = Linear(ff_size, hidden_size)\n",
    "        self.dropout = Dropout(p=dropout_ratio)\n",
    "        self.gelu = GeLU()        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        output = self.dropout(x)\n",
    "        return output\n",
    "    \n",
    "# Embedding\n",
    "class Embedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "        if padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight[padding_idx].fill_(0)\n",
    "            # 注册梯度钩子来防止padding_idx位置的权重更新\n",
    "            self.weight.register_hook(self._backward_hook)\n",
    "        \n",
    "        \n",
    "    def _backward_hook(self, grad):\n",
    "        if self.padding_idx is not None:\n",
    "            grad[self.padding_idx].fill_(0)\n",
    "        return grad\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if (x < 0).any() or (x >= self.num_embeddings).any():\n",
    "            raise ValueError(f\"indices must be in the range [0, {self.num_embeddings-1}]\")\n",
    "        # x.shape: [B, L]\n",
    "        # 直接 fancy-indexing\n",
    "        # output.shape: [B, L, embedding_dim]\n",
    "        output = self.weight[x]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer Encoder\n",
    "Transformer Encoder一共有两个子层，分别是 **多头注意力子层** 和 **前馈神经网络子层**，每个子层都包含了 残差连接 和 层归一化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 512])\n",
      "output.shape == [batch_size, seq_len, d_model]: True\n"
     ]
    }
   ],
   "source": [
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size: int, \n",
    "                 num_heads: int, \n",
    "                 ff_size: int,\n",
    "                 max_seq_len: int=512,  \n",
    "                 dropout_ratio: float=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(num_heads, hidden_size, max_seq_len)\n",
    "        self.ffn = FeedForward(hidden_size, ff_size, dropout_ratio)\n",
    "        self.ln1 = LayerNormalization(hidden_size)\n",
    "        self.ln2 = LayerNormalization(hidden_size)\n",
    "    def forward(self, x, src_padding_mask=None):\n",
    "        # 多头注意力子层（Post Norm）\n",
    "        attention_output = self.mha(x, padding_mask=src_padding_mask)\n",
    "        x = self.ln1(x + attention_output)\n",
    "        # 前馈神经网络子层（Post Norm）\n",
    "        ffn_output = self.ffn(x)\n",
    "        output = self.ln2(x + ffn_output)\n",
    "        return output\n",
    "\n",
    "# Encoder堆叠\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int,\n",
    "                 hidden_size: int,\n",
    "                 num_heads: int, \n",
    "                 ff_size: int,\n",
    "                 max_seq_len: int=512,  \n",
    "                 dropout_ratio: float=0.1):\n",
    "        super().__init__()\n",
    "        self.encoderlayers = nn.ModuleList([\n",
    "            EncoderLayer(\n",
    "                hidden_size, \n",
    "                num_heads, \n",
    "                ff_size, \n",
    "                max_seq_len, \n",
    "                dropout_ratio\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x, src_padding_mask=None):\n",
    "        for layer in self.encoderlayers:\n",
    "            x = layer(x, src_padding_mask)\n",
    "        return x\n",
    "\n",
    "# 测试\n",
    "n_layers=6\n",
    "d_model=512\n",
    "n_heads=8\n",
    "d_ff=2048\n",
    "max_seq_len=16\n",
    "dropout_ratio=0.1\n",
    "batch_size = 32\n",
    "seq_len = max_seq_len\n",
    "\n",
    "encoder = Encoder(\n",
    "    n_layers,\n",
    "    d_model,\n",
    "    n_heads,\n",
    "    d_ff,\n",
    "    max_seq_len,\n",
    "    dropout_ratio\n",
    ")\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output = encoder(x)  # shape: [batch_size, seq_len, d_model]\n",
    "print(output.shape)\n",
    "print(f\"output.shape == [batch_size, seq_len, d_model]: {list(output.shape) == [batch_size, seq_len, d_model]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Decoder\n",
    "Transformer Decoder一共有三个子层，分别是 **掩码多头注意力子层**、**编码器-解码器注意力子层** 和 **前馈神经网络子层**，每个子层都包含了 残差连接 和 层归一化；\n",
    "\n",
    "- **掩码多头注意力子层**使用了causal mask，用上三角矩阵实现，保证每个位置只能看见自己及之前的位置。\n",
    "- **编码器-解码器注意力子层**的Q来自decoder上一个**掩码多头注意力子层**的输出，而K和V来自最后一个 encoder layer 的输出，这使得 Transformer Decoder 能够同时关注**当前输出序列的上下文信息（Q）** 和 **输入序列的编码信息（K、V）**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16, 512])\n",
      "decoder_output.shape == [batch_size, tgt_seq_len, d_model]: True\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_heads: int,\n",
    "        max_seq_len: int,\n",
    "        ff_size: int,\n",
    "        dropout_ratio: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(num_heads, hidden_size, max_seq_len, dropout_ratio)\n",
    "        self.cross_attention = MultiHeadCrossAttention(num_heads, hidden_size, max_seq_len, dropout_ratio)\n",
    "        self.ff = FeedForward(hidden_size, ff_size, dropout_ratio)\n",
    "        self.ln1 = LayerNormalization(hidden_size)\n",
    "        self.ln2 = LayerNormalization(hidden_size)\n",
    "        self.ln3 = LayerNormalization(hidden_size)\n",
    "    \n",
    "    def forward(self, x, encoder_output, causal_mask=None, decoder_padding_mask=None, encoder_attention_mask=None):\n",
    "        # self-attention\n",
    "        self_attention_output = self.self_attention(x, causal_mask, decoder_padding_mask)\n",
    "        x = self.ln1(x + self_attention_output)\n",
    "        \n",
    "        # cross-attention\n",
    "        cross_attention_output = self.cross_attention(x, encoder_output, encoder_attention_mask)\n",
    "        x = self.ln2(x + cross_attention_output)\n",
    "        \n",
    "        # feed forward\n",
    "        ff_output = self.ff(x)\n",
    "        output = self.ln3(x + ff_output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        hidden_size: int,\n",
    "        num_heads: int, \n",
    "        ff_size: int,\n",
    "        max_seq_len: int=512,  \n",
    "        dropout_ratio: float=0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decoderlayers = nn.ModuleList([\n",
    "            DecoderLayer(\n",
    "                hidden_size,\n",
    "                num_heads,\n",
    "                max_seq_len,\n",
    "                ff_size,\n",
    "                dropout_ratio\n",
    "            ) for _ in range(0, num_layers)\n",
    "        ])\n",
    "    def forward(self, x, encoder_output, causal_mask=None, decoder_padding_mask=None, encoder_attention_mask=None):\n",
    "        for layer in self.decoderlayers:\n",
    "            x = layer(x, encoder_output, causal_mask, decoder_padding_mask, encoder_attention_mask)\n",
    "        return x\n",
    "\n",
    "# 测试\n",
    "n_layers = 6\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "max_seq_len = 16\n",
    "dropout_ratio = 0.1\n",
    "batch_size = 32\n",
    "tgt_seq_len = max_seq_len\n",
    "src_seq_len = max_seq_len\n",
    "\n",
    "decoder = Decoder(\n",
    "    n_layers,\n",
    "    d_model,\n",
    "    n_heads,\n",
    "    d_ff,\n",
    "    max_seq_len,\n",
    "    dropout_ratio\n",
    ")\n",
    "\n",
    "decoder_input = torch.randn(batch_size, tgt_seq_len, d_model)\n",
    "encoder_output = torch.randn(batch_size, src_seq_len, d_model)\n",
    "\n",
    "causal_mask = torch.triu(torch.ones(tgt_seq_len, tgt_seq_len), diagonal=1).bool()\n",
    "# print(causal_mask)\n",
    "encoder_attention_mask = torch.ones(batch_size, src_seq_len).bool()    # 这里其实不mask任何位置\n",
    "decoder_attention_mask = torch.ones(batch_size, tgt_seq_len).bool()    # 这里其实不mask任何位置\n",
    "\n",
    "decoder_output = decoder(\n",
    "    decoder_input, \n",
    "    encoder_output, \n",
    "    causal_mask, \n",
    "    decoder_attention_mask,\n",
    "    encoder_attention_mask\n",
    ")\n",
    "\n",
    "print(decoder_output.shape)\n",
    "print(f\"decoder_output.shape == [batch_size, tgt_seq_len, d_model]: {list(decoder_output.shape) == [batch_size, tgt_seq_len, d_model]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer\n",
    "组合Embedding, Encoder Block, Decoder Block 实现完整的Transformer，用于Seq2Seq任务（如机器翻译）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Sequence shape: torch.Size([32, 16])\n",
      "Target Sequence shape: torch.Size([32, 16])\n",
      "Output shape: torch.Size([32, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        src_vocab_size: int, \n",
    "        tgt_vocab_size: int, \n",
    "        hidden_size: int, \n",
    "        num_heads: int, \n",
    "        ff_size: int, \n",
    "        max_seq_len: int=512, \n",
    "        num_layers: int=6,\n",
    "        padding_idx: int=0,\n",
    "        dropout_ratio: float=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.src_embedding = Embedding(src_vocab_size, hidden_size, padding_idx)\n",
    "        self.tgt_embedding = Embedding(tgt_vocab_size, hidden_size, padding_idx)\n",
    "        self.encoder = Encoder(num_layers, hidden_size, num_heads, ff_size, max_seq_len, dropout_ratio)\n",
    "        self.decoder = Decoder(num_layers, hidden_size, num_heads, ff_size, max_seq_len, dropout_ratio)\n",
    "        self.output_proj = Linear(hidden_size, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = Dropout(dropout_ratio)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def create_causal_mask(self, seq_len, device):\n",
    "        return torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1).bool()\n",
    "    \n",
    "    def create_padding_mask(self, x, pad_idx):\n",
    "        return x != pad_idx\n",
    "    \n",
    "    def forward(self, ori_src, ori_tgt):\n",
    "        \"\"\"\n",
    "        src.shape: [batch_size, src_seq_len]\n",
    "        tgt.shape: [batch_size, tgt_seq_len]\n",
    "        \"\"\"\n",
    "        batch_size, src_seq_len = ori_src.shape\n",
    "        _ , tgt_seq_len = ori_tgt.shape\n",
    "        \n",
    "        # masking\n",
    "        src_padding_mask = self.create_padding_mask(ori_src, self.padding_idx)\n",
    "        tgt_padding_mask = self.create_padding_mask(ori_tgt, self.padding_idx)\n",
    "        # decoder的self-attention层需要因果掩码\n",
    "        tgt_causal_mask = self.create_causal_mask(tgt_seq_len, ori_tgt.device)\n",
    "        \n",
    "        # src encoding\n",
    "        src_embed = self.src_embedding(ori_src)    # [batch_size, src_seq_len, hidden_size]\n",
    "        src_embed = self.dropout(src_embed)\n",
    "        encoder_output = self.encoder(src_embed, src_padding_mask)   # [batch_size, src_seq_len, hidden_size]\n",
    "        \n",
    "        # tgt decoding\n",
    "        tgt_embed = self.tgt_embedding(ori_tgt)    # [batch_size, tgt_seq_len, hidden_size]\n",
    "        tgt_embed = self.dropout(tgt_embed)\n",
    "        decoder_output = self.decoder(             # [batch_size, tgt_seq_len, hidden_size]\n",
    "            tgt_embed,\n",
    "            encoder_output,\n",
    "            tgt_causal_mask,\n",
    "            tgt_padding_mask,\n",
    "            src_padding_mask\n",
    "        )\n",
    "        \n",
    "        # proj\n",
    "        output = self.output_proj(decoder_output)  # [batch_size, tgt_seq_len, tgt_vocab_size]\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 测试\n",
    "n_layers = 6\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "max_seq_len = 16\n",
    "dropout_ratio = 0.1\n",
    "batch_size = 32\n",
    "tgt_seq_len = max_seq_len\n",
    "src_seq_len = max_seq_len\n",
    "\n",
    "vocab_size= 512\n",
    "\n",
    "# source seq & target seq\n",
    "src = torch.randint(0, vocab_size, (batch_size, src_seq_len))\n",
    "tgt = torch.randint(0, vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "myTransformer = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    hidden_size=d_model,\n",
    "    num_heads=n_heads,\n",
    "    ff_size = d_ff,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_layers=n_layers,\n",
    "    padding_idx=0,\n",
    "    dropout_ratio=0.1\n",
    ")\n",
    "\n",
    "output = myTransformer(src, tgt)\n",
    "\n",
    "print(\"Source Sequence shape:\", src.shape)\n",
    "print(\"Target Sequence shape:\", tgt.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
