{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 七、手撕Embedding层\n",
    "Embedding用于将离散的输入(单词、字符)转换为连续的向量表示。\n",
    "\n",
    "公式: $$y = W[idx]$$\n",
    "\n",
    "其中$idx$是输入索引张量，$W$是嵌入矩阵。如果输入$idx$的形状是$[\\text{batch\\_size}, \\text{seq\\_length}]$，那么:\n",
    "\n",
    "- 嵌入矩阵$W$的形状是$[\\text{num\\_embeddings}, \\text{embedding\\_dim}]$\n",
    "- 输出$y$的形状是$[\\text{batch\\_size}, \\text{seq\\_length}, \\text{embedding\\_dim}]$\n",
    "\n",
    "`padding_idx=k`: 初始化时将指定的embedding(第k行)置为0，并在训练时显式地阻止对应参数更新，使其保持零向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyEmbedding(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(num_embeddings, embedding_dim))\n",
    "        if padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.weight[padding_idx].fill_(0)\n",
    "            # 注册梯度钩子来防止padding_idx位置的权重更新\n",
    "            self.weight.register_hook(self._backward_hook)\n",
    "        \n",
    "    def _backward_hook(self, grad):\n",
    "        if self.padding_idx is not None:\n",
    "            grad[self.padding_idx].fill_(0)\n",
    "        return grad\n",
    "                \n",
    "    def forward(self, x):\n",
    "        if (x < 0).any() or (x >= self.num_embeddings).any():\n",
    "            raise ValueError(f\"indices must be in the range [0, {self.num_embeddings-1}]\")\n",
    "        # x.shape: [B, L]\n",
    "        # 直接 fancy-indexing\n",
    "        # output.shape: [B, L, embedding_dim]\n",
    "        output = self.weight[x]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Embedding output shape: torch.Size([3, 5, 8])\n",
      "PyTorch Embedding output shape: torch.Size([3, 5, 8])\n",
      "My Embedding output at padding position [0,0]:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "PyTorch Embedding output at padding position [0,0]:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "My Embedding output at padding position [1,2]:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "PyTorch Embedding output at padding position [1,2]:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SelectBackward0>)\n",
      "PyTorch gradient at padding_idx: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "My gradient at padding_idx: tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "PyTorch gradient at index 1: tensor([ 3.3747,  3.7849,  1.3418, -3.5435,  2.1515, -3.6038, -0.8204, -2.5532])\n",
      "My gradient at index 1: tensor([ 3.3747,  3.7849,  1.3418, -3.5435,  2.1515, -3.6038, -0.8204, -2.5532])\n",
      "Non-padding gradients are similar: True\n"
     ]
    }
   ],
   "source": [
    "batch_size = 3\n",
    "seq_length = 5\n",
    "num_embeddings = 10\n",
    "embedding_dim = 8\n",
    "padding_idx = 0\n",
    "\n",
    "x = torch.randint(0, num_embeddings, (batch_size, seq_length))\n",
    "# 添加padding索引\n",
    "x[0, 0] = padding_idx\n",
    "x[1, 2] = padding_idx\n",
    "\n",
    "# 使用相同的初始权重创建两个嵌入层\n",
    "initial_weight = torch.randn(num_embeddings, embedding_dim)\n",
    "initial_weight[padding_idx].fill_(0)\n",
    "\n",
    "my_embedding = MyEmbedding(num_embeddings=num_embeddings, \n",
    "                          embedding_dim=embedding_dim, \n",
    "                          padding_idx=padding_idx)\n",
    "with torch.no_grad():\n",
    "    my_embedding.weight.copy_(initial_weight)\n",
    "\n",
    "torch_embedding = nn.Embedding(num_embeddings=num_embeddings, \n",
    "                              embedding_dim=embedding_dim, \n",
    "                              padding_idx=padding_idx)\n",
    "with torch.no_grad():\n",
    "    torch_embedding.weight.copy_(initial_weight)\n",
    "\n",
    "my_output = my_embedding(x)\n",
    "torch_output = torch_embedding(x)\n",
    "\n",
    "# 打印输出形状\n",
    "print(f\"My Embedding output shape: {my_output.shape}\")\n",
    "print(f\"PyTorch Embedding output shape: {torch_output.shape}\")\n",
    "# 检查输出中padding位置的嵌入向量是否为零\n",
    "print(f\"My Embedding output at padding position [0,0]:\\n{my_output[0,0]}\")\n",
    "print(f\"PyTorch Embedding output at padding position [0,0]:\\n{torch_output[0,0]}\")\n",
    "print(f\"My Embedding output at padding position [1,2]:\\n{my_output[1,2]}\")\n",
    "print(f\"PyTorch Embedding output at padding position [1,2]:\\n{torch_output[1,2]}\")\n",
    "\n",
    "# 创建相同的目标张量用于计算损失\n",
    "target = torch.randn_like(my_output)\n",
    "\n",
    "my_loss = (my_output - target).pow(2).sum()\n",
    "my_loss.backward()\n",
    "my_grad_at_padding = my_embedding.weight.grad[padding_idx].clone()  # 保存梯度以供后续比较\n",
    "\n",
    "# 由于在同一个计算图中运行两个模型，当第一个模型的反向传播发生时，PyTorch会计算所有需要梯度的张量的梯度\n",
    "# 因此需要清除torch_embedding的梯度\n",
    "torch_embedding.zero_grad()\n",
    "torch_output = torch_embedding(x)\n",
    "torch_loss = (torch_output - target).pow(2).sum()\n",
    "torch_loss.backward()\n",
    "\n",
    "# 验证padding_idx处的梯度是否都为零\n",
    "print(f\"PyTorch gradient at padding_idx: {torch_embedding.weight.grad[padding_idx]}\")\n",
    "print(f\"My gradient at padding_idx: {my_grad_at_padding}\")\n",
    "# 验证非padding_idx处的梯度是否相似\n",
    "other_idx = 1  # 选择一个非padding的索引进行比较\n",
    "print(f\"PyTorch gradient at index {other_idx}: {torch_embedding.weight.grad[other_idx]}\")\n",
    "print(f\"My gradient at index {other_idx}: {my_embedding.weight.grad[other_idx]}\")\n",
    "print(f\"Non-padding gradients are similar: {torch.allclose(torch_embedding.weight.grad[1:], my_embedding.weight.grad[1:], rtol=1e-5)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
